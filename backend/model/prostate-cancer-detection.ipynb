{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18647,"databundleVersionId":1126921,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **About This Notebook**\nIn this notebook , I will start with complete explanation of everything you need know related to Prostate Cancer and its detection and I will explain the dataset and then create a Model to detect Prostate Cancer.\n# **I) Medical Point Of View:** \nIn this section, we willtalk about the general medical knowledge that you need to know about Prostate Cancer.\n\n**1) What is Prostate Cancer?**\n\nProstate is a walnut-sized gland located just below the bladder and in front of the rectum in men. It plays a crucial role in the reproductive system by producing a fluid that, along with sperm from the testicles, makes up semen. The prostate surrounds the urethra, the tube that carries urine from the bladder and semen from the reproductive system out through the penis.\n\nProstate cancer is a condition where cells in the prostate gland multiply uncontrollably, forming a tumor. In its early stages, it may not cause noticeable harm, but as it progresses, it can lead to problems like difficulty urinating and discomfort. Without intervention, it can potentially spread to other parts of the body, becoming more dangerous. \n\n**2) Prostate biopsy test:**\n\nWhen the doctor detects an abnomalies in your prostate after examine it with inital tests,\nhe collects a sample of prostate tissue.\nProstate biopsy is often done using a thin needle that's inserted into the prostate to collect tissue. The tissue sample is analyzed in a lab to determine whether cancer cells are present.\n\n**3) GLEASON score:**\n\nAfter a biopsy confirms cancer, the subsequent stage involves assessing the aggressiveness (grade) of the cancer cells. A laboratory pathologist analyzes a cancer sample to gauge the extent of differentiation from healthy cells. A higher grade signifies a more aggressive cancer with an increased likelihood of rapid spreading. The Gleason score, commonly employed to assess prostate cancer cells, combines two numbers and spans from 2 (less aggressive) to 10 (highly aggressive), although the lower end of the range is less frequently utilized.\n\n**4) ISUP grade:**\n\nAccording to current guidelines by the International Society of Urological Pathology (ISUP), the Gleason scores are summarized into an ISUP grade on a scale from 1 to 5 according to the following rule:\n\nGleason score 6 = ISUP grade 1 \n\nGleason score 7 (3 + 4) = ISUP grade 2 \n\nGleason score 7 (4 + 3) = ISUP grade 3 \n\nGleason score 8 = ISUP grade 4 \n\nGleason score 9-10 = ISUP grade 5 \n\nIf there is no cancer in the sample, we use the label ISUP grade 0 in this competition.\n\n**5) GLEASON score Samples**\n\n[A]Benign prostate glands with folded epithelium :The cytoplasm is pale and the nuclei small and regular. The glands are grouped together.\n\n[B]Prostatic adenocarcinoma : Gleason Pattern 3 has no loss of glandular differentiation. Small glands infiltrate between benign glands. The cytoplasm is often dark and the nuclei enlarged with dark chromatin and some prominent nucleoli. Each epithelial unit is separate and has a lumen.\n\n[C]Prostatic adenocarcinoma : Gleason Pattern 4 has partial loss of glandular differentiation. There is an attempt to form lumina but the tumor fails to form complete, well-developed glands. This microphotograph shows irregular cribriform cancer, i.e. epithelial sheets with multiple lumina. There are also some poorly formed small glands and some fused glands. All of these are included in Gleason Pattern 4.\n\n[D]Prostatic adenocarcinoma : Gleason Pattern 5 has an almost complete loss of glandular differentiation. Dispersed single cancer cells are seen in the stroma. Gleason Pattern 5 may also contain solid sheets or strands of cancer cells. All microphotographs show hematoxylin and eosin stains at 20x lens magnification.\n","metadata":{}},{"cell_type":"code","source":"## import os\nimport os\n# There are two ways to load the data from the PANDA dataset:\n# Option 1: Load images using openslide\nimport openslide\n# Option 2: Load images using skimage (requires that tifffile is installed)\nimport skimage.io\nimport random\nimport seaborn as sns\nimport cv2\n\n# General packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageStat\nfrom IPython.display import Image, display\n\n# Plotly for the interactive viewer (see last section)\nimport plotly.graph_objs as go\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:49:55.326285Z","iopub.execute_input":"2024-03-31T21:49:55.326649Z","iopub.status.idle":"2024-03-31T21:49:55.332326Z","shell.execute_reply.started":"2024-03-31T21:49:55.326607Z","shell.execute_reply":"2024-03-31T21:49:55.331310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:49:58.353547Z","iopub.execute_input":"2024-03-31T21:49:58.353913Z","iopub.status.idle":"2024-03-31T21:49:58.359336Z","shell.execute_reply.started":"2024-03-31T21:49:58.353881Z","shell.execute_reply":"2024-03-31T21:49:58.358296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Location of the training images\n\nBASE_PATH = '../input/prostate-cancer-grade-assessment'\n\n# image and mask directories\ndata_dir = f'{BASE_PATH}/train_images'\nmask_dir = f'{BASE_PATH}/train_label_masks'\n\n\n# Location of training labels\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv').set_index('image_id')\ntest = pd.read_csv(f'{BASE_PATH}/test.csv')\nsubmission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:49:59.229160Z","iopub.execute_input":"2024-03-31T21:49:59.229823Z","iopub.status.idle":"2024-03-31T21:49:59.284095Z","shell.execute_reply.started":"2024-03-31T21:49:59.229788Z","shell.execute_reply":"2024-03-31T21:49:59.283354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head())\nprint(\"Shape of training data :\", train.shape)\nprint(\"unique data provider :\", len(train.data_provider.unique()))\nprint(\"unique isup_grade(target) :\", len(train.isup_grade.unique()))\nprint(\"unique gleason_score :\", len(train.gleason_score.unique()))\nprint(\"unique gleason_score :\", train.gleason_score.unique())\nprint(\"unique isup_grade(target) :\", train.isup_grade.unique())\n\n# Count the number of images per 'isup_grade' (class)\nimages_per_isup_grade = train['isup_grade'].value_counts()\nprint(\"\\nNumber of images per ISUP grade (class):\")\nprint(images_per_isup_grade)\n\n# Optionally, if you also want to count images per 'gleason_score'\nimages_per_gleason_score = train['gleason_score'].value_counts()\nprint(\"\\nNumber of images per Gleason score:\")\nprint(images_per_gleason_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:49:59.855774Z","iopub.execute_input":"2024-03-31T21:49:59.856451Z","iopub.status.idle":"2024-03-31T21:49:59.889928Z","shell.execute_reply.started":"2024-03-31T21:49:59.856419Z","shell.execute_reply":"2024-03-31T21:49:59.889066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display images from an array of IDs\ndef display_images(slides): \n    f, ax = plt.subplots(5,3, figsize=(18,22))\n    for i, slide in enumerate(slides):\n        image = openslide.OpenSlide(os.path.join(data_dir, f'{slide}.tiff'))\n        spacing = 1 / (float(image.properties['tiff.XResolution']) / 10000)\n        patch = image.read_region((1780,1950), 0, (256, 256))\n        ax[i//3, i%3].imshow(patch) \n        image.close()       \n        ax[i//3, i%3].axis('off')\n        \n        image_id = slide\n        data_provider = train.loc[slide, 'data_provider']\n        isup_grade = train.loc[slide, 'isup_grade']\n        gleason_score = train.loc[slide, 'gleason_score']\n        ax[i//3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:50:00.510431Z","iopub.execute_input":"2024-03-31T21:50:00.511165Z","iopub.status.idle":"2024-03-31T21:50:00.519097Z","shell.execute_reply.started":"2024-03-31T21:50:00.511135Z","shell.execute_reply":"2024-03-31T21:50:00.518067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6 Quickly displaying few images**\n\nIn the following sections we will load data from the slides with OpenSlide. The benefit of OpenSlide is that we can load arbitrary regions of the slide, without loading the whole image in memory. Want to interactively view a slide? We have added an interactive viewer to this notebook in the last section.\n\nYou can read more about the OpenSlide python bindings in the documentation: https://openslide.org/api/python/\n\n","metadata":{}},{"cell_type":"code","source":"images = [\n    '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n    '037504061b9fba71ef6e24c48c6df44d',\n    '035b1edd3d1aeeffc77ce5d248a01a53',\n    '059cbf902c5e42972587c8d17d49efed',\n    '06a0cbd8fd6320ef1aa6f19342af2e68',\n    '06eda4a6faca84e84a781fee2d5f47e1',\n    '0a4b7a7499ed55c71033cefb0765e93d',\n    '0838c82917cd9af681df249264d2769c',\n    '046b35ae95374bfb48cdca8d7c83233f',\n    '074c3e01525681a275a42282cd21cbde',\n    '05abe25c883d508ecc15b6e857e59f32',\n    '05f4e9415af9fdabc19109c980daf5ad',\n    '060121a06476ef401d8a21d6567dee6d',\n    '068b0e3be4c35ea983f77accf8351cc8',\n    '08f055372c7b8a7e1df97c6586542ac8'\n]\n\ndisplay_images(images)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:50:01.558480Z","iopub.execute_input":"2024-03-31T21:50:01.558854Z","iopub.status.idle":"2024-03-31T21:50:04.911835Z","shell.execute_reply.started":"2024-03-31T21:50:01.558824Z","shell.execute_reply":"2024-03-31T21:50:04.910489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **II) Building the model:** ","metadata":{}},{"cell_type":"markdown","source":"**1) Collecting data**","metadata":{}},{"cell_type":"code","source":"def is_blank(patch, threshold=240):\n    \"\"\"Check if the patch is mostly blank (white)\"\"\"\n    stat = ImageStat.Stat(patch)\n    mean_brightness = sum(stat.mean) / 3  # Calculate mean brightness of the patch\n    return mean_brightness > threshold","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:50:04.914181Z","iopub.execute_input":"2024-03-31T21:50:04.914615Z","iopub.status.idle":"2024-03-31T21:50:04.919560Z","shell.execute_reply.started":"2024-03-31T21:50:04.914587Z","shell.execute_reply":"2024-03-31T21:50:04.918766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a DataFrame 'train' with necessary details\n# train = pd.read_csv('path_to_your_csv')\n\ncollected_data_output_dir = f'./collected_images'\n\nlow_tiss_slides = [ '033e39459301e97e457232780a314ab7',\n                    '0b6e34bf65ee0810c1a4bf702b667c88',\n                    '3385a0f7f4f3e7e7b380325582b115c9',\n                    '3790f55cad63053e956fb73027179707',\n                    '5204134e82ce75b1109cc1913d81abc6',\n                    'a08e24cff451d628df797efc4343e13c',\n                    '379ZxmoZZwyfLZnABDtMdh3nwjqps6fov7'\n                  ]\nwhite_patches = 0\nwidth, height = 256, 256  # Patch size\nincrease_factor = 2  # Factor to increase shift distance\n\nfor index, row in train.iterrows():\n    if index in low_tiss_slides:\n        continue\n    slide = index\n    image_path = os.path.join(data_dir, f'{slide}.tiff')\n    image = openslide.OpenSlide(image_path)\n    \n    # Slide dimensions\n    slide_width, slide_height = image.dimensions\n    \n    # Initial coordinates for the patch and initial shift\n    x, y = 1780, 1950\n    shift_x, shift_y = 100, 100  # Initial shift values\n    \n    patch = image.read_region((x, y), 0, (width, height)).convert(\"RGB\")\n    \n    attempts = 0  # Keep track of attempts to find a non-blank patch\n\n    while is_blank(patch):  # Limit attempts to prevent infinite loop\n        x += shift_x\n        y += shift_y\n        \n        # Ensure x and y do not exceed slide dimensions\n        if x + width > slide_width or y + height > slide_height:\n            x = max(0, min(slide_width - width, x))  # Adjust x within bounds\n            y = max(0, min(slide_height - height, y))  # Adjust y within bounds\n            shift_x *= increase_factor  # Increase shift size\n            shift_y *= increase_factor  # Increase shift size\n            \n        # Extract a new patch with the adjusted coordinates\n        patch = image.read_region((x, y), 0, (width, height)).convert(\"RGB\")\n        \n        # If we've adjusted the shift beyond a reasonable limit, stop trying for this slide\n        if shift_x >= slide_width or shift_y >= slide_height:\n            break\n        \n        attempts += 1\n    \n    if is_blank(patch):  # If still blank after adjustments, skip this slide\n        white_patches+=1\n        continue\n    \n    # Define the directory based on the gleason_score\n    isup = train.loc[slide, 'isup_grade']  # Replace '+' with '_' to avoid directory issues\n    label_dir = os.path.join(collected_data_output_dir, f'isup_{isup}')\n    os.makedirs(label_dir, exist_ok=True)\n    \n    # Save the extracted patch\n    patch.save(os.path.join(label_dir, f'{slide}.png'))\n    image.close()\n    \nprint(f\"No suitable patch found for {white_patches} slides\")","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:50:11.449774Z","iopub.execute_input":"2024-03-31T21:50:11.450104Z","iopub.status.idle":"2024-03-31T23:03:23.726101Z","shell.execute_reply.started":"2024-03-31T21:50:11.450080Z","shell.execute_reply":"2024-03-31T23:03:23.725184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2) Loading Data**","metadata":{}},{"cell_type":"code","source":"# Global declarations for data\nbatch_size = 32\nimage_size = (256, 256)\nseed = 123  # Seed for reproducibility\n\n# Load the training dataset\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(\n    directory=collected_data_output_dir,\n    validation_split=0.2,  # Use 20% of the images as the test set\n    subset=\"training\",\n    seed=seed,\n    image_size=image_size,\n    batch_size=batch_size,\n    label_mode='categorical'  # Use 'categorical' for multi-class classification\n)\n\n# Load the test dataset\ntest_dataset = tf.keras.utils.image_dataset_from_directory(\n    directory=collected_data_output_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=seed,\n    image_size=image_size,\n    batch_size=batch_size,\n    label_mode='categorical'\n)\n\n# The datasets are now ready for training and evaluation\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T23:04:09.870457Z","iopub.execute_input":"2024-03-31T23:04:09.870822Z","iopub.status.idle":"2024-03-31T23:04:13.810064Z","shell.execute_reply.started":"2024-03-31T23:04:09.870793Z","shell.execute_reply":"2024-03-31T23:04:13.809361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3) Creating Block functions**","metadata":{}},{"cell_type":"code","source":"def conv_block(filters, kernel_size, activation='relu', pool_size=(2, 2), use_batchnorm=True):\n    block = tf.keras.Sequential()\n    block.add(tf.keras.layers.Conv2D(filters, kernel_size, padding='same', use_bias=not use_batchnorm))\n    if use_batchnorm:\n        block.add(tf.keras.layers.BatchNormalization())\n    block.add(tf.keras.layers.Activation(activation))\n    block.add(tf.keras.layers.MaxPooling2D(pool_size=pool_size))\n    return block\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T23:04:23.534353Z","iopub.execute_input":"2024-03-31T23:04:23.535282Z","iopub.status.idle":"2024-03-31T23:04:23.541720Z","shell.execute_reply.started":"2024-03-31T23:04:23.535247Z","shell.execute_reply":"2024-03-31T23:04:23.540829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dense_block(units, activation='relu', dropout_rate=None, use_batchnorm=True):\n    block = tf.keras.Sequential()\n    block.add(tf.keras.layers.Dense(units, use_bias=not use_batchnorm))\n    if use_batchnorm:\n        block.add(tf.keras.layers.BatchNormalization())\n    block.add(tf.keras.layers.Activation(activation))\n    if dropout_rate:\n        block.add(tf.keras.layers.Dropout(dropout_rate))\n    return block","metadata":{"execution":{"iopub.status.busy":"2024-03-31T23:04:25.500748Z","iopub.execute_input":"2024-03-31T23:04:25.501095Z","iopub.status.idle":"2024-03-31T23:04:25.507120Z","shell.execute_reply.started":"2024-03-31T23:04:25.501069Z","shell.execute_reply":"2024-03-31T23:04:25.506210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4) Reassembling the modal**","metadata":{}},{"cell_type":"code","source":"num_classes = 6\n\nmodel = tf.keras.Sequential([\n    # Add convolutional blocks\n    tf.keras.Input(shape=(image_size[0],image_size[1], 3)),\n    conv_block(256, (3, 3)),\n    conv_block(128, (3, 3)),\n    conv_block(64, (3, 3)),\n    conv_block(32, (3, 3)),\n    \n    \n    tf.keras.layers.GlobalAveragePooling2D(),\n    \n    # Add dense blocks\n    dense_block(64, dropout_rate=0),\n    \n    # Output layer\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n])\n\nlearning_rate = 0.001\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n# Compile the model\nmodel.compile(optimizer=optimizer,\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(),\n                       tf.keras.metrics.Recall()])\n\nfor x, y in train_dataset.take(1):\n    print(x.shape, y.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T23:04:26.929495Z","iopub.execute_input":"2024-03-31T23:04:26.929974Z","iopub.status.idle":"2024-03-31T23:04:28.160103Z","shell.execute_reply.started":"2024-03-31T23:04:26.929936Z","shell.execute_reply":"2024-03-31T23:04:28.159196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5) Evaluation**","metadata":{}},{"cell_type":"code","source":"history=model.fit(train_dataset,validation_data=test_dataset,epochs=15, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T23:04:33.696361Z","iopub.execute_input":"2024-03-31T23:04:33.697162Z","iopub.status.idle":"2024-03-31T23:16:08.438975Z","shell.execute_reply.started":"2024-03-31T23:04:33.697125Z","shell.execute_reply":"2024-03-31T23:16:08.438166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(15)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T21:26:08.719874Z","iopub.execute_input":"2024-03-31T21:26:08.720660Z","iopub.status.idle":"2024-03-31T21:26:09.102640Z","shell.execute_reply.started":"2024-03-31T21:26:08.720627Z","shell.execute_reply":"2024-03-31T21:26:09.101690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}